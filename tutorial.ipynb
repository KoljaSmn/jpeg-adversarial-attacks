{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d28c2e8f-9793-42d3-87a4-b5ac025968f7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Adversarial Perturbations straight on JPEG Coefficients: Tutorial\n",
    "\n",
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b8d7ba-d78d-46b7-ac73-cfcaf7808026",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from adversarial_attacks.config.config import Config\n",
    "Config.VERBOSE = 0\n",
    "Config.RECONSTRUCT_ORIGINAL_DATASETS = False # Set to True if original datasets should be recreated and not loaded from file\n",
    "Config.RECONSTRUCT_ADVERSARIAL_DATASETS = False # Set to True if adversarial datasets should be recreated and not loaded from file\n",
    "#Config.LOG_FILE = None  # Uncomment to show logs in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ef3a9f-92c0-4ae0-87f5-328319c16407",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import adversarial_attacks.main\n",
    "adversarial_attacks.main.init(use_cpu=False, run_eagerly=False, gpu_nrs=[0], tf_strategy='mirrored') \n",
    "# gpu_nrs allows to set which gpus to use; tf_strategy is used for (adversarial) training, options are mirrored or default; set use_cpu=True to run on cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4580bd55-ec7a-4de7-baff-f4a654aed264",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import adversarial_attacks.utils.visualization as vis_utils\n",
    "import adversarial_attacks.utils.general as general_utils\n",
    "import adversarial_attacks.utils.transformation as transformation_utils\n",
    "import adversarial_attacks.utils.jpeg\n",
    "from adversarial_attacks.datasets.original import RGBDataset, JpegDataset, YCbCrDataset\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from copy import deepcopy\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7c7463-787a-435f-850b-695efef09dbe",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Datasets\n",
    "\n",
    "The data can be loaded as RGB data, or JPEG data, or YCbCr Pixel data.\n",
    "The attacks themself use RGB data as input, which is then converted to e.g. JPEG data within the attacks call function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49060f8e-7f30-4e6e-8e56-37b3b7220714",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = 'cifar10' # 'imagenet' or 'cifar10' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d3d405-5a6f-4624-b20d-e1dbd3f3ae53",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rgb_dataset = RGBDataset(ds, 'validation' if ds == 'imagenet' else 'test', shuffle=False, repeat=False, number_of_images=None) # number_of_images=None -> full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31863276-bf79-4014-aead-5bc7ec3a6a7c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We visualize two batches of size 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b40c14-4597-4e8a-8ba1-f42c78ebed2c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for images, labels in rgb_dataset.ds.batch(8).take(2): # rgb_dataset.ds is a tf.data.Dataset\n",
    "    vis_utils.show_multiple_images(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b95e4c-e2e4-4b56-813e-933b898bdeab",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For a JpegDataset, the entries look as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9d3407-c273-450b-940c-9183dbee9484",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "jpeg_dataset = JpegDataset(ds, 'validation' if ds == 'imagenet' else 'test', shuffle=False, repeat=False, number_of_images=None, jpeg_quality=100, chroma_subsampling=False) # number_of_images=None -> full dataset\n",
    "for (Y, Cb, Cr), labels in jpeg_dataset.ds.batch(8).take(1):\n",
    "    print(tf.shape(Y), tf.shape(Cb), tf.shape(Cr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ffaf40-5fff-4b9e-b6e8-4734bb1bb517",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "jq = 100\n",
    "chroma_subsampling = False\n",
    "\n",
    "jpeg_dataset = JpegDataset(ds, 'validation' if ds == 'imagenet' else 'test', shuffle=False, repeat=False, number_of_images=None, jpeg_quality=jq, chroma_subsampling=chroma_subsampling) # number_of_images=None -> full dataset\n",
    "for (Y, Cb, Cr), labels in jpeg_dataset.ds.batch(8).take(1):\n",
    "    rgb = transformation_utils.jpeg_to_rgb_batch((Y, Cb, Cr), ds, jpeg_quality=jq, chroma_subsampling=chroma_subsampling)\n",
    "    vis_utils.show_multiple_images(rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a268276-02b8-43c3-9c7e-9abbc288d0ac",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Models and (Adversarial) Training\n",
    "\n",
    "You can either load one of our models our define a new one. To load an existing tf model, use the adversarial_attacks.models.models.Model class.\n",
    "The model must then be saved under {Config.MODEL_PATH}/{ds}/{load_model_name}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e224e33-1a2b-4c81-b7b9-5328cd4dea13",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from adversarial_attacks.models.models import Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a60921-96b5-4766-a3cd-21f30d4131db",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "load_model_name, save_model_name = 'Resnet', 'Resnet_Test'\n",
    "m = Model(ds, load_model_name=load_model_name, save_model_name=save_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a388f093-4a69-4d0c-b767-a6c2aba4970a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This model can then be trained with the original dataset using the train_original_ds function. All our models expect RGB input. Thus, the RGB dataset is used.\n",
    "The model will be saved automatically under save_model_name if the val_loss is improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8661104-04fd-4b40-9a30-4651f1275739",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_train, ds_test = RGBDataset(ds, 'train', shuffle=True, repeat=True), RGBDataset(ds, 'validation' if ds == 'imagenet' else 'test', shuffle=True, repeat=True)\n",
    "#m.train_original_ds(ds_train, ds_test, batch_size=8, epochs=1, optimizer_lambda=lambda: tf.keras.optimizers.legacy.SGD(0.1, momentum=0.9, decay=0.0001))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8865896-3c1d-4783-b883-83cec0298e79",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "An own model can best be defined by inheriting from the adversarial_attacks.models.models.Model class. \n",
    "Make sure that the Model expects 0...255 input and returns logits. In our models, we included tensorflow's preproccess input function in the model.\n",
    "In this case, we use a model predefined and pretrained in Tensorflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7837b4-c1bb-4e84-8f14-d2f6d2e0e285",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TutorialModel(Model):\n",
    "    def __init__(self, save_model_name='tutorial_model', load_model_name=None, save_model=False, jpeg_compression_quality=None):\n",
    "        \"\"\"\n",
    "        When load_model_name is not None, the save model will be used instead of building a new model.\n",
    "        If jpeg_compression_quality is not None, JPEG compression will be added to the start of the model. Note that this is not differentiable.\n",
    "        \"\"\"\n",
    "        super().__init__(dataset_name='imagenet', save_model_name=save_model_name, load_model_name=load_model_name, save_model=save_model, jpeg_compression_quality=jpeg_compression_quality)\n",
    "        \n",
    "    def build_model(self):\n",
    "        inp = tf.keras.Input(shape=(224, 224, 3))\n",
    "        preprocessed_input = inp\n",
    "        if self.jpeg_compression_quality is not None:\n",
    "            preprocessed_input = jpeg_compression_for_rgb_model(self.ds_name, preprocessed_input,\n",
    "                                                                self.jpeg_compression_quality)\n",
    "        \n",
    "        preprocessed_input = tf.keras.applications.efficientnet.preprocess_input(preprocessed_input)\n",
    "        tf_inception_v3 = tf.keras.applications.efficientnet.EfficientNetB0(include_top=True, weights='imagenet',\n",
    "                                                                         input_shape=(224, 224, 3),\n",
    "                                                                         classifier_activation=None,\n",
    "                                                                         input_tensor=preprocessed_input)\n",
    "        return tf_inception_v3\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b5efc2-43a5-4438-910c-9a323f3c61cf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "m = TutorialModel(save_model_name='tutorial_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5612014d-f145-4346-b279-6c7483e43729",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The model can also be adversarially trained. Every Combination of our attacks can be used. Here, we used two JPEG and one RGB attack, all are weighted equally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed92839-6536-4175-bef0-caed29290b63",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from adversarial_attacks.attacks.rgb import RGBBIM\n",
    "from adversarial_attacks.attacks.jpeg import JpegBIM\n",
    "from adversarial_attacks.utils.frequency_masks import lambdas\n",
    "\n",
    "lambda_unmasked = lambdas['unmasked'][0]\n",
    "lambda_medium = lambdas['medium'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82716e6d-ca37-43bc-9349-5795d62709be",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "chroma_subsampling = False\n",
    "fix_zero=False\n",
    "jq = 100\n",
    "\n",
    "dynamic_attacks = {\n",
    "    JpegBIM(ds, None, model=m, eps_Y = 0.9, eps_Cb = 0.9, eps_Cr=0.9, alpha_Y=0.9/4., alpha_Cb=0.9/4, alpha_Cr=0.9/4., T=7, lambda_Y=lambda_medium, lambda_Cb=lambda_medium, lambda_Cr=lambda_medium, chroma_subsampling=chroma_subsampling, fix_zero_coefficients=fix_zero, jpeg_quality=jq, random_start=True): 1., \n",
    "    JpegBIM(ds, None, model=m, eps_Y = 0.4, eps_Cb = 0.4, eps_Cr=0.4, alpha_Y=0.4/4., alpha_Cb=0.4/4, alpha_Cr=0.4/4., T=7, lambda_Y=lambda_unmasked, lambda_Cb=lambda_unmasked, lambda_Cr=lambda_unmasked, chroma_subsampling=chroma_subsampling, fix_zero_coefficients=fix_zero, jpeg_quality=jq, random_start=True): 1.,\n",
    "    RGBBIM(ds, None, model=m, epsilon=8., alpha=2., T=7): 1.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31033c9-5ee5-4104-99a8-4706a7bafa85",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from adversarial_attacks.datasets.adversarial_training_datasets import AdversarialTrainingDataset\n",
    "\n",
    "batch_size = 10\n",
    "ds_train_rgb = adversarial_attacks.datasets.original.RGBDataset(ds, 'train', augmentation=0, shuffle=False, cache=False)\n",
    "ds_test_rgb = adversarial_attacks.datasets.original.RGBDataset(ds, 'validation' if ds == 'imagenet' else 'test', shuffle=False, cache=False)\n",
    "adversarial_ds_train = AdversarialTrainingDataset(ds_train_rgb, dynamic_attacks, batch_size, shuffle=True)\n",
    "adversarial_ds_test = AdversarialTrainingDataset(ds_test_rgb, dynamic_attacks, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17daed8d-724f-4e4e-88af-84782ef417be",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "m.train_adversarial_ds(adversarial_ds_train, adversarial_ds_test, epochs=2, optimizer_lambda=lambda: tf.keras.optimizers.RMSprop(1e-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eb39cc-011d-4e5c-96f4-c7bf1a2617b4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Attacks and Experiments\n",
    "\n",
    "Now, we will show some examples of attacked images and then, how success rates and perceptual distances can be measured for experiments.\n",
    "When only executing the attack on few images, it should be quicker to enable eager execution in the initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909a0703-a910-4884-bdda-c15a8669ba8f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from adversarial_attacks.attacks.rgb import RGBBIM\n",
    "from adversarial_attacks.attacks.jpeg import JpegBIM\n",
    "from adversarial_attacks.utils.frequency_masks import lambdas\n",
    "\n",
    "\n",
    "ds = 'imagenet' # 'imagenet', 'cifar10'\n",
    "number_of_images = 200 # 10000 is used in the paper's experiments\n",
    "\n",
    "source_model = 'Resnet'\n",
    "\n",
    "lambda_medium = lambdas['medium'][0] # the medium vector that is also used in the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e0aa26-0da8-4129-ab9f-1185b0c5e483",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Sample Images\n",
    "\n",
    "First, we will show some sample images for our JPEG luma medium and the RGB attack (both BIM). Below the images, we also compute the LPIPS distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da253f18-e719-4cc6-a413-e1149eb6f5c1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rgb_ds = RGBDataset(ds, train_or_test='test' if ds == 'cifar10' else 'validation', shuffle=False, repeat=False, number_of_images=None) # \n",
    "images, labels = next(rgb_ds.ds.batch(8).__iter__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0636221d-e947-44d5-8630-8757001aae99",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from adversarial_attacks.models.lpips import LossNetwork\n",
    "\n",
    "ln = LossNetwork(ds, lpips=True) # is already trained and uses a vgg16 net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f49b45-8fe4-4fc6-b6a4-483404569fbb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vis_utils.show_multiple_images(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5028f492-a6f6-4901-b0fc-a4012d7a5932",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We start with the RGB attack.\n",
    "\n",
    "For both the RGB and the JPEG attack, we use a very high epsilon value such that the difference in the structure of the perturbations becomes clear. \n",
    "The amount of perturbation is also not comparable between the two attacks.\n",
    "So, it does not say something about the attacks' efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012ef3b6-d22a-40b0-b181-1599e0137511",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "example_attack = RGBBIM(dataset=ds, model_name=source_model, epsilon=64., T=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc90009f-6812-4a83-91ae-2c27e959768c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "adv_images = example_attack(images, labels)\n",
    "vis_utils.show_multiple_images(adv_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc0391b-3056-47c7-a321-cb56ada6bc46",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ln(images, adv_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a112f721-4398-486e-a15e-743903c28872",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, the JPEG attack follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6790c4-2c9a-482e-8366-1445bf4caf37",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "example_attack = JpegBIM(dataset=ds, model_name=source_model, eps_Y=10., eps_Cb=0., eps_Cr=0., lambda_Y=lambda_medium, T=10, chroma_subsampling=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b7fdf9-d0cb-4c5c-9d2b-6f451f6cb0c0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "adv_images = example_attack(images, labels)\n",
    "vis_utils.show_multiple_images(adv_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e39015a-e91c-4c42-aef5-4d603242aaf0",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ln(images, adv_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53e3c53-c904-4e0d-95b8-d5ef940bcd33",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Experiments\n",
    "\n",
    "This section will be on our Experiments. As explained in the paper, we incrementally increase the input parameter (perturbation bound - epsilon), and measure both the perceptual distance (LPIPS, but also CIEDE2000 $L_2$, RGB $L_2$) and the attack's success rate (as well as the nets accuracy and crossentropy loss). \n",
    "\n",
    "First, we define the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defd8fc1-9bd5-435f-90c7-79d6022fb5b0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rgb_ds = RGBDataset(ds, train_or_test='test' if ds == 'cifar10' else 'validation', shuffle=False, repeat=False, number_of_images=number_of_images) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720c0f57-a5f6-4e7f-bb47-ffe65504787f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, we define a set of evaluation metrics and distance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5a0ba8-19e9-4d7d-ad58-a0c3d1a4a192",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First, we define (target-) models on which the adversarial examples should be evaluated \n",
    "# In this example, we only use an undefended model and one defended with JPEG compression at inference time.\n",
    "model_names_for_evaluation = ['Densenet', 'Densenet_Compression_75'] # make sure that each tf model expects 0...255 input and is saved in {Config.MODEL_PATH}/{ds}/{model_name}\n",
    "\n",
    "\n",
    "# define evaluation_metrics that will measure the success rate etc on a given model\n",
    "from adversarial_attacks.utils.evaluation_metrics import EvaluationMetricCollection\n",
    "evaluation_metrics = {model_name: EvaluationMetricCollection(ds, model_name) for model_name in model_names_for_evaluation}\n",
    "\n",
    "# define distance_metric that will measure the perceptual distances\n",
    "from adversarial_attacks.utils.distance_metrics import ExperimentDistanceWrapper\n",
    "distance_metrics = ExperimentDistanceWrapper(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727c4e97-c57f-4aa8-8455-7bc1b0d38ce7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, we define a dictionary of attacks. Each entry consists of a list of attacks, with different parameters, e.g. epsilons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c89a77-cbfe-4a58-8c18-eca751581e09",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "attack_dict = {\n",
    " 'rgb_bim': [RGBBIM(dataset=ds, model_name=source_model, epsilon=eps, T=10) for eps in [2., 8., 16., 32.]],\n",
    " 'jpeg_luma_medium_bim':  [JpegBIM(dataset=ds, model_name=source_model, eps_Y=eps, eps_Cb=0., eps_Cr=0., lambda_Y=lambda_medium, T=10, fix_zero_coefficients=True) for eps in [1., 3., 5.]]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486d486f-dbf1-43b6-be3e-343ae0b4b550",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "distance_metrics_results, attack_results = {}, {} # sucess rates and distances will be saved here\n",
    "\n",
    "import math\n",
    "n_batches = math.ceil(number_of_images/batch_size) # compute the number of batches\n",
    "\n",
    "for attack_name in tqdm(attack_dict.keys(), desc='Iterating attack names...'):\n",
    "    distance_metrics_results[attack_name] = []\n",
    "    attack_results[attack_name] = []\n",
    "    \n",
    "    for attack in tqdm(attack_dict[attack_name], desc=f'Iterating {attack_name} attacks...'):\n",
    "        for images, labels in tqdm(rgb_ds.ds.batch(batch_size), total=n_batches, leave=False, desc='Attacking Batches...'):\n",
    "            adv_images = attack(images, labels) # execute the attack\n",
    "            distance_metrics.update_state(images, adv_images) # measure and update the perceptual distances\n",
    "            for evaluation_metric in evaluation_metrics.values():\n",
    "                evaluation_metric.update_state(labels, images, adv_images) # for every target model, evaluate the images: measure success rate, accuracy and crossentropy loss\n",
    "        attack_results[attack_name].append({model_name: evaluation_metric.result() for model_name, evaluation_metric in evaluation_metrics.items()}) # write the attack evaluation to the attack_results dict\n",
    "        for evaluation_metric in evaluation_metrics.values():\n",
    "            evaluation_metric.reset_state() # reset the evaluation metrics\n",
    "        distance_metrics_results[attack_name].append(distance_metrics.result()) # write distances to the distance_metrics_results dict\n",
    "        distance_metrics.reset_state() # reset the distance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91857754-1564-4158-ace5-f4e12eb94b4a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_results_metric(data, attack_name, metric, model):\n",
    "    res = []\n",
    "    for entry in data[attack_name]:\n",
    "        res.append(entry[model][metric])\n",
    "    return res\n",
    "\n",
    "def get_distance_metric(data, attack_name, metric, norm='l2'):\n",
    "    res = []\n",
    "    for entry in data[attack_name]:\n",
    "        if metric == 'perceptual':\n",
    "            res.append(entry[metric]['avg'])\n",
    "        else:\n",
    "            res.append(entry[metric][norm]['avg'])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6251b2f8-0293-4ab9-9974-b83a8931ef2e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We now plot the success rate (or Accuracy, Loss) in dependence of the perceptual distance, measured by LPIPS (or CIEDE2000, or RGB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab62ac4-24a3-4d93-bca5-5aa8404bf191",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f788265-95dc-479d-b0ac-caf834564633",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "metric = 'perceptual' # 'perceptual', 'ciede2000', 'rgb'\n",
    "norm = 'l2' # ignored if metric=='perceptual'\n",
    "\n",
    "success_metric = 'success_rate' # 'Acc', 'Loss'\n",
    "target_model = 'Densenet_Compression_75'\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "for attack_name in attack_dict:\n",
    "    plt.plot(get_distance_metric(distance_metrics_results, attack_name, metric, norm), get_results_metric(attack_results, attack_name, success_metric, target_model), label=attack_name, marker='x')\n",
    "    \n",
    "plt.legend()\n",
    "\n",
    "plt.title(f'Attack Efficiency on the {target_model}')\n",
    "plt.xlabel('LPIPS' if metric == 'perceptual' else f'{metric} - {norm}')\n",
    "plt.ylabel(success_metric)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d113ed51-5301-45ff-b51d-a96d073e5f0d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The plot shows that the JPEG luma medium attack is more efficient on the Densenet_Compression_75 than the RGB BIM attack, as stated in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764b5d72-4379-4556-8d3a-c0e68d25101f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}